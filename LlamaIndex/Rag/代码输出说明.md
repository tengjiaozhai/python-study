# AssessmentAccuary.py 代码输出逐行说明

## 输出与源码对应关系

### 1. 分隔线和标题
```
============================================================
RAG系统评估报告
============================================================
```

**对应源码（第 82-84 行）：**
```python
print("=" * 60)
print("RAG系统评估报告")
print("=" * 60)
```
- `"=" * 60` 生成 60 个等号作为分隔线
- 打印标题"RAG系统评估报告"

---

### 2. 问题和答案信息
```
问题: 王芳是哪个部门的？
标准答案: 王芳是教研部的教研专员
系统回答: 王芳是教研部的。
```

**对应源码（第 93-97 行）：**
```python
question = "王芳是哪个部门的？"
response = query_engine.query(question)

print(f"\n问题: {question}")
print(f"标准答案: 王芳是教研部的教研专员")
print(f"系统回答: {response}")
```
- 第 93 行：定义问题变量
- 第 94 行：调用 RAG 系统查询，返回 response 对象
- 第 96-98 行：打印问题、标准答案和系统回答
- `response` 对象在打印时会自动调用 `__str__()` 方法，显示回答文本

---

### 3. 检索到的上下文
```
检索到的上下文:
1. # 公司各部门职责内容...
```

**对应源码（第 100-105 行）：**
```python
print(f"\n检索到的上下文:")
for i, source_node in enumerate(response.source_nodes, 1):
    content = source_node.get_content()
    # 只显示前100个字符，避免输出过长
    preview = content[:100] + "..." if len(content) > 100 else content
    print(f"{i}. {preview}")
```
- 第 100 行：打印标题
- 第 101 行：遍历 `response.source_nodes`（检索到的文档片段）
  - `enumerate(response.source_nodes, 1)` 从 1 开始编号
- 第 102 行：获取每个节点的文本内容
- 第 104 行：如果内容超过 100 字符，只显示前 100 个字符加省略号
- 第 105 行：打印编号和内容预览

**说明：**
- `response.source_nodes` 是一个列表，包含了 RAG 系统检索到的相关文档片段
- 每个 `source_node` 对象包含文档内容、元数据、相似度分数等信息

---

### 4. 评估指标标题
```
评估指标:
----------------------------------------
```

**对应源码（第 107-108 行）：**
```python
print(f"\n评估指标:")
print("-" * 40)
```
- 打印"评估指标:"标题
- 打印 40 个短横线作为分隔

---

### 5. Ragas 评估进度条
```
Evaluating: 100%|███████████████████| 1/1 [00:14<00:00, 14.83s/it]
```

**对应源码（第 110 行）：**
```python
result = evaluate_result(question, response, ground_truth)
```

这个进度条是由 `evaluate_result` 函数内部调用的 `ragas.evaluate()` 自动生成的：

**对应源码（第 52-56 行）：**
```python
# 第4步：使用 Ragas 评估
score = evaluate(
    dataset=dataset, metrics=[answer_correctness], llm=llm, embeddings=embeddings
)
```
- `ragas.evaluate()` 函数内部使用 `tqdm` 库显示评估进度
- `1/1` 表示评估 1 个样本，共 1 个样本
- `14.83s/it` 表示每个样本评估耗时约 14.83 秒

---

### 6. 评估结果表格
```
  user_input                                 retrieved_contexts  response    reference  answer_correctness
0  王芳是哪个部门的？  [# 公司各部门职责内容...]  王芳是教研部的。  王芳是教研部的教研专员            0.735745
```

**对应源码（第 111 行）：**
```python
print(result)
```

这个表格是 `evaluate_result` 函数返回的 pandas DataFrame：

**对应源码（第 58 行）：**
```python
return score.to_pandas()
```

**表格列说明：**
- `user_input`: 用户输入的问题（来自第 40 行的 `"question": [question]`）
- `retrieved_contexts`: 检索到的上下文列表（来自第 43 行的 `"contexts": [contexts]`）
- `response`: 系统的回答（来自第 41 行的 `"answer": [answer]`）
- `reference`: 标准答案（来自第 42 行的 `"ground_truth": [ground_truth]`）
- `answer_correctness`: 答案正确性得分（由 ragas 的 `answer_correctness` 指标计算）

---

### 7. 答案正确性得分
```
答案正确性得分: 0.7357
```

**对应源码（第 113-114 行）：**
```python
answer_correctness_score = result["answer_correctness"].values[0]
print(f"\n答案正确性得分: {answer_correctness_score:.4f}")
```
- 第 113 行：从 DataFrame 中提取 `answer_correctness` 列的第一个值
- 第 114 行：格式化输出，保留 4 位小数（`.4f`）

---

### 8. 综合评价
```
综合评价: 良好
```

**对应源码（第 116-124 行）：**
```python
if answer_correctness_score >= 0.9:
    evaluation = "优秀"
elif answer_correctness_score >= 0.7:
    evaluation = "良好"
elif answer_correctness_score >= 0.5:
    evaluation = "中等"
else:
    evaluation = "需要改进"

print(f"综合评价: {evaluation}")
```
- 根据得分范围判断评价等级：
  - ≥ 0.9: 优秀
  - ≥ 0.7: 良好
  - ≥ 0.5: 中等
  - < 0.5: 需要改进
- 本例中得分 0.7357，属于"良好"

---

### 9. 结束分隔线
```
============================================================
```

**对应源码（第 125 行）：**
```python
print("=" * 60)
```
- 打印 60 个等号作为结束分隔线

---

## 核心函数流程

### `evaluate_result()` 函数详解

```python
def evaluate_result(question, response, ground_truth):
    # 步骤1：提取回答文本
    if hasattr(response, "response_txt"):
        answer = response.response_txt
    else:
        answer = str(response)
    
    # 步骤2：提取检索到的上下文
    contexts = [source_node.get_content() for source_node in response.source_nodes]
    
    # 步骤3：构造评估数据集
    data_samples = {
        "question": [question],
        "answer": [answer],
        "ground_truth": [ground_truth],
        "contexts": [contexts],
    }
    dataset = Dataset.from_dict(data_samples)
    
    # 步骤4：调用 ragas 评估
    score = evaluate(
        dataset=dataset,
        metrics=[answer_correctness],
        llm=llm,
        embeddings=embeddings
    )
    
    # 步骤5：返回 pandas DataFrame
    return score.to_pandas()
```

**关键点：**
1. `response` 对象包含：
   - 回答文本（通过 `str(response)` 获取）
   - 检索到的文档片段（`response.source_nodes`）
   
2. `ragas.evaluate()` 需要特定格式的数据：
   - `question`: 问题列表
   - `answer`: 系统回答列表
   - `ground_truth`: 标准答案列表
   - `contexts`: 检索上下文列表
   
3. `answer_correctness` 指标会：
   - 使用 LLM 比较系统回答和标准答案的语义相似度
   - 使用 embeddings 计算文本向量的相似度
   - 综合给出 0-1 之间的得分

---

## 总结

整个程序的输出流程：
1. 打印报告标题
2. 执行 RAG 查询
3. 显示问题、标准答案、系统回答
4. 显示检索到的上下文片段
5. 调用 ragas 进行评估（显示进度条）
6. 显示评估结果表格
7. 提取并显示答案正确性得分
8. 根据得分给出综合评价
9. 打印结束分隔线

每一行输出都对应代码中的特定语句，通过理解这些对应关系，可以更好地掌握 RAG 评估系统的工作原理。
