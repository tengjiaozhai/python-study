# å‡†ç¡®æ€§è¯„ä¼°
import os
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ Utils
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from ragas import evaluate
from ragas.metrics import answer_correctness
from Rag.Utils import get_ragas_models, llama_llm, llama_embed_model
from datasets import Dataset
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex

# é…ç½® LlamaIndex å…¨å±€è®¾ç½®
Settings.llm = llama_llm
Settings.embed_model = llama_embed_model

# è·å– ragas è¯„ä¼°ç”¨çš„ LLM å’Œ Embeddings
llm, embeddings = get_ragas_models()


# å®šä¹‰è¯„ä¼°å‡½æ•°
def evaluate_result(question, response, ground_truth):
    """
    è¯„ä¼° RAG ç³»ç»Ÿçš„å›ç­”è´¨é‡
    
    å‚æ•°:
        question: ç”¨æˆ·æå‡ºçš„é—®é¢˜
        response: RAG ç³»ç»Ÿè¿”å›çš„å“åº”å¯¹è±¡
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
    
    è¿”å›:
        è¯„ä¼°ç»“æœçš„ DataFrame
    """
    # ç¬¬1æ­¥ï¼šè·å–å›ç­”å†…å®¹
    # response å¯¹è±¡å¯èƒ½æœ‰ response_txt å±æ€§ï¼Œæˆ–è€…ç›´æ¥è½¬å­—ç¬¦ä¸²
    if hasattr(response, "response_txt"):
        answer = response.response_txt
    else:
        answer = str(response)

    # ç¬¬2æ­¥ï¼šè·å–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    # response.source_nodes åŒ…å«äº†æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    contexts = [source_node.get_content() for source_node in response.source_nodes]

    # ç¬¬3æ­¥ï¼šæ„é€ è¯„ä¼°æ•°æ®é›†
    # Ragas éœ€è¦ç‰¹å®šæ ¼å¼çš„æ•°æ®
    data_samples = {
        "question": [question],  # é—®é¢˜åˆ—è¡¨
        "answer": [answer],  # ç³»ç»Ÿå›ç­”åˆ—è¡¨
        "ground_truth": [ground_truth],  # æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
        "contexts": [contexts],  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
    }
    dataset = Dataset.from_dict(data_samples)
    
    # è¾“å‡ºè¯„ä¼°æ•°æ®é›†è¡¨æ ¼
    print("\nğŸ“‹ è¯„ä¼°æ•°æ®é›†:")
    print("-" * 60)
    # ä½¿ç”¨pandas DataFrameæ ¼å¼åŒ–è¾“å‡º
    import pandas as pd
    df = pd.DataFrame({
        'é—®é¢˜': data_samples['question'],
        'ç³»ç»Ÿå›ç­”': data_samples['answer'],
        'æ ‡å‡†ç­”æ¡ˆ': data_samples['ground_truth'],
        'æ£€ç´¢æ–‡æ¡£æ•°': [len(data_samples['contexts'][0])]
    })
    print(df.to_string(index=False))

    # ç¬¬4æ­¥ï¼šä½¿ç”¨ Ragas è¯„ä¼°
    score = evaluate(
        dataset=dataset, metrics=[answer_correctness], llm=llm, embeddings=embeddings
    )

    return score.to_pandas()


# ============================================================
# ä¸»ç¨‹åºï¼šæ‰§è¡Œ RAG æ£€ç´¢å’Œè¯„ä¼°
# ============================================================

print("=" * 60)
print("RAGç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š")
print("=" * 60)

# ç¬¬1æ­¥ï¼šåŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
doc_dir = os.path.join(current_dir, "doc")
documents = SimpleDirectoryReader(input_dir=doc_dir).load_data()
index = VectorStoreIndex.from_documents(documents)

# ç¬¬2æ­¥ï¼šåˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(similarity_top_k=4)

# ç¬¬3æ­¥ï¼šæ‰§è¡ŒæŸ¥è¯¢
question = "ç‹èŠ³æ˜¯å“ªä¸ªéƒ¨é—¨çš„ï¼Ÿ"
response = query_engine.query(question)

# ç¬¬4æ­¥ï¼šæ‰“å°é—®é¢˜å’Œç³»ç»Ÿå›ç­”
print(f"\né—®é¢˜: {question}")
print(f"æ ‡å‡†ç­”æ¡ˆ: ç‹èŠ³æ˜¯æ•™ç ”éƒ¨çš„æ•™ç ”ä¸“å‘˜")
print(f"ç³»ç»Ÿå›ç­”: {response}")

# ç¬¬5æ­¥ï¼šæ‰“å°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
print(f"\næ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:")
for i, source_node in enumerate(response.source_nodes, 1):
    content = source_node.get_content()
    # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
    preview = content[:100] + "..." if len(content) > 100 else content
    print(f"{i}. {preview}")

# ç¬¬6æ­¥ï¼šæ‰§è¡Œè¯„ä¼°
print(f"\nè¯„ä¼°æŒ‡æ ‡:")
print("=" * 60)
ground_truth = "ç‹èŠ³æ˜¯æ•™ç ”éƒ¨çš„æ•™ç ”ä¸“å‘˜"
result = evaluate_result(question, response, ground_truth)

# ç¬¬7æ­¥ï¼šç¾åŒ–è¾“å‡ºè¯„ä¼°ç»“æœè¡¨æ ¼
print("\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœè¡¨æ ¼:")
print("=" * 60)

# è®¾ç½® pandas æ˜¾ç¤ºé€‰é¡¹
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 50)

# åˆ›å»ºä¸€ä¸ªæ›´æ˜“è¯»çš„ç»“æœè¡¨æ ¼
result_display = result.copy()

# å¦‚æœæœ‰ contexts åˆ—ï¼Œç®€åŒ–æ˜¾ç¤º
if 'retrieved_contexts' in result_display.columns:
    result_display['retrieved_contexts'] = result_display['retrieved_contexts'].apply(
        lambda x: f"[{len(x)} ä¸ªæ–‡æ¡£ç‰‡æ®µ]" if isinstance(x, list) else str(x)[:30] + "..."
    )

# ç®€åŒ– response å’Œ reference åˆ—çš„æ˜¾ç¤º
if 'response' in result_display.columns:
    result_display['response'] = result_display['response'].apply(
        lambda x: str(x)[:40] + "..." if len(str(x)) > 40 else str(x)
    )
if 'reference' in result_display.columns:
    result_display['reference'] = result_display['reference'].apply(
        lambda x: str(x)[:40] + "..." if len(str(x)) > 40 else str(x)
    )

print(result_display.to_string(index=False))

# ç¬¬8æ­¥ï¼šæå–å¹¶æ˜¾ç¤ºå„é¡¹æŒ‡æ ‡
print("\n" + "=" * 60)
print("ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯¦æƒ…:")
print("=" * 60)

answer_correctness_score = result["answer_correctness"].values[0]

# åˆ›å»ºæŒ‡æ ‡æ±‡æ€»è¡¨
metrics_summary = pd.DataFrame({
    'æŒ‡æ ‡åç§°': ['ç­”æ¡ˆæ­£ç¡®æ€§ (Answer Correctness)'],
    'å¾—åˆ†': [f'{answer_correctness_score:.4f}'],
    'ç™¾åˆ†æ¯”': [f'{answer_correctness_score * 100:.2f}%'],
    'è¯„çº§': ['ä¼˜ç§€' if answer_correctness_score >= 0.9 
             else 'è‰¯å¥½' if answer_correctness_score >= 0.7 
             else 'ä¸­ç­‰' if answer_correctness_score >= 0.5 
             else 'éœ€è¦æ”¹è¿›']
})

print(metrics_summary.to_string(index=False))

# ç¬¬9æ­¥ï¼šç­”æ¡ˆå¯¹æ¯”è¡¨æ ¼
print("\n" + "=" * 60)
print("ï¿½ ç­”æ¡ˆå¯¹æ¯”åˆ†æ):")
print("=" * 60)

comparison_df = pd.DataFrame({
    'ç±»å‹': ['æ ‡å‡†ç­”æ¡ˆ', 'ç³»ç»Ÿå›ç­”'],
    'å†…å®¹': [ground_truth, str(response)],
    'é•¿åº¦': [len(ground_truth), len(str(response))],
    'åŒ¹é…åº¦': ['-', f'{answer_correctness_score * 100:.2f}%']
})
print(comparison_df.to_string(index=False))

# ç¬¬10æ­¥ï¼šç»™å‡ºç»¼åˆè¯„ä»·å’Œå»ºè®®
print("\n" + "=" * 60)
print("ğŸ’¡ ç»¼åˆè¯„ä»·:")
print("=" * 60)

if answer_correctness_score >= 0.9:
    evaluation = "ä¼˜ç§€ â­â­â­â­â­"
    suggestion = "ç³»ç»Ÿå›ç­”éå¸¸å‡†ç¡®ï¼Œä¸æ ‡å‡†ç­”æ¡ˆé«˜åº¦ä¸€è‡´ã€‚"
    emoji = "ğŸ‰"
elif answer_correctness_score >= 0.7:
    evaluation = "è‰¯å¥½ â­â­â­â­"
    suggestion = "ç³»ç»Ÿå›ç­”åŸºæœ¬å‡†ç¡®ï¼Œä½†å¯èƒ½åœ¨ç»†èŠ‚ä¸Šä¸æ ‡å‡†ç­”æ¡ˆæœ‰äº›å·®å¼‚ã€‚"
    emoji = "ğŸ‘"
elif answer_correctness_score >= 0.5:
    evaluation = "ä¸­ç­‰ â­â­â­"
    suggestion = "ç³»ç»Ÿå›ç­”éƒ¨åˆ†æ­£ç¡®ï¼Œå»ºè®®ä¼˜åŒ–æ£€ç´¢ç­–ç•¥æˆ–å¢åŠ ç›¸å…³æ–‡æ¡£ã€‚"
    emoji = "âš ï¸"
else:
    evaluation = "éœ€è¦æ”¹è¿› â­â­"
    suggestion = "ç³»ç»Ÿå›ç­”ä¸æ ‡å‡†ç­”æ¡ˆå·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦æ£€æŸ¥æ–‡æ¡£è´¨é‡å’Œæ£€ç´¢ç®—æ³•ã€‚"
    emoji = "âŒ"

print(f"{emoji} è¯„çº§: {evaluation}")
print(f"ğŸ“ å»ºè®®: {suggestion}")
print(f"ğŸ“Š å‡†ç¡®ç‡: {answer_correctness_score * 100:.2f}%")

# ç¬¬11æ­¥ï¼šæ£€ç´¢è´¨é‡åˆ†æ
print("\n" + "=" * 60)
print("ğŸ“š æ£€ç´¢è´¨é‡åˆ†æ:")
print("=" * 60)
print(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(response.source_nodes)}")
print(f"ä½¿ç”¨çš„æ–‡æ¡£æ•°é‡: {len([node for node in response.source_nodes])}")

# æ˜¾ç¤ºæ¯ä¸ªæ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆå¦‚æœæœ‰åˆ†æ•°ï¼‰
if hasattr(response.source_nodes[0], 'score') and response.source_nodes[0].score is not None:
    print("\nå„æ–‡æ¡£ç›¸å…³æ€§å¾—åˆ†:")
    for i, node in enumerate(response.source_nodes, 1):
        print(f"  æ–‡æ¡£ {i}: {node.score:.4f}")
else:
    print("(æœªæä¾›æ–‡æ¡£ç›¸å…³æ€§å¾—åˆ†)")

print("=" * 60)
